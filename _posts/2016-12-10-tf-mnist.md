---
title:      "Handwritten digits recognition with Tensorflow"
---

<p>
    We are going to build a simple neural network to recognize handwritten digits (MNIST database) with Tensorflow.
    <br> If you have time and motivation I recommend you to watch this inspirational <a href="https://www.youtube.com/watch?v=vq2nnJ4g6N0">video</a>.
    This topic is mostly a summary of it and TensorFlow tutorial.
</p>

<p>
    This is a <b>classification problem</b> : we <b>learn</b> from datas and then try to guess the class of a new data : that is called <b>test</b>.
    <br> Since we know the class of each data, we say that we make a <u>supervised learning</u> algorithm.
</p>

<h3 class="subsection-heading"> Datas </h3>

<p>
    We'll use the famous MNIST handwritten digits datasets. There are 60,000 training examples and 10,000 test examples.
    <br> You can find the <a href="http://yann.lecun.com/exdb/mnist/">MNIST dataset on Yann Lecun site.</a> 
    <br> The images are all 28x28 representing numbers from 0 to 9.        
</p>
<p>
    The classes of data are digits from 0 to 9. Given a new data we will try to recognize which digit it represents.
    <br><br><img class="center-image" src="{{ site.baseurl }}/img/2016-12-10-tf-mnist/mnist-2.png" alt="mnist 2" height="100" width="100">
</p>
<p class="caption">It's a 2 ! </p>

<h3 class="subsection-heading"> Let's build our model ! </h3>

<p>
    Since we want to give the class of the digit, we need an output layer of 10 coefficients, each of it telling us the probability of being 0 or 1 or 2...
    <br>We will consider <b>all pixels</b> of the image : a 28x28 pixel matrix.
</p>
<p>
    The activation function is a logistic regression : y = ax + b.
</p>
<p>
    We now have a model, that takes the input value, consider a bias, and calculates the probability of having one digit.
    <br> Given Y as output vector, X as input vector, W the weight matrix and b a bias vector :
    <br> <img class="center-image" src="{{ site.baseurl }}/img/2016-12-10-tf-mnist/linear.png" alt="linear output">
</p>

<h4 class="subsubsection-heading"> Softmax </h4>
<p>
    Softmax is a simple function that squashes the range of a vector to [0,1]. It's easier to deal with probabilities with it.
    <br> In Neural net we call it an <u>activation function</u>. All these functions are non-linear.
    <br> <img class="center-image" src="{{ site.baseurl }}/img/2016-12-10-tf-mnist/softmax.png" alt="softmax">
</p>

<h4 class="subsubsection-heading"> Gradient descent </h4>
<p>
    Gradient descent is an algorithm to minimize a function of a Hilbert space.
    <br> Intuitively, if you want to minimize a function, the shortest path is to go to the opposite of the local gradient.
    <br> At each iteration we go to :
    <img class="center-image" src="{{ site.baseurl }}/img/2016-12-10-tf-mnist/gradient_descent.png" alt="gradient descent">
</p>

<h3 class="subsection-heading"> Code </h3>

<p>
    Resources :
    <ul>
    <li> The <a href="https://www.tensorflow.org/api_docs/python/">API</a> of TensorFlow. </li>
    <li> The <a href="https://github.com/tensorflow/tensorflow">Github repo</a> is awesome and contains tutorials. Most of my code comes from here. </li>
    </ul>
</p>

<p>
    First, we must define Y and minimize it through some iterations.
    <br> We use TF placeholders represents a variables that will be feed during the execution. Here we use it to define input 
    and output vectors.
    <br> We will treat <u>100 images</u> simultaneously to have more generalization during the learning process.
</p>

<p>
    Think about dimensions of vectors and matrix. Remember that you must have the same dimensions when adding 2 vectors and 
    [a,b]x[b,c] when multiplying.
    <br> There we flatten the input in 784 to multiply with weights.
    <br> To build the model we simply use Tf matrix multiplication operator and softmax function.
</p>

{% highlight python linenos %}
  x = tf.placeholder(tf.float32, [None, 784])
  W = tf.Variable(tf.zeros([784, 10]))
  b = tf.Variable(tf.zeros([10]))
  # The model
  y = tf.matmul(x, W) + b
  y_ = tf.placeholder(tf.float32, [None, 10])
{% endhighlight %}

<p>
    (you've probably noticed the '_' after 'y'. It represents the correct output vector, that will be compared to the 
    presumed output 'y', since we know the stuff in our supervised examples).
</p>
<p>
    Now we need a way to give a mark to a predicted result : we use an <u>entropy computation</u>.
</p>

{% highlight python linenos %}
  cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(y, y_))
  train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)
{% endhighlight %}

<p>
    In TF you define a program in 2 steps : you define your model and minimization first, and then you run a <b>session</b>
    <br> Usually, compiler run "main" function and all the program, but TF has been thought to work in multiples GPUs.
    That's why it defines a computation graph and run when you ask it for. 
</p>

{% highlight python linenos %}
  sess = tf.InteractiveSession()
  tf.global_variables_initializer().run()
{% endhighlight %}

<p>
    Train the model : We put 100 images at each iteration in our learning function.
    <br> This is a imple loop :
</p>

{% highlight python linenos %}
  for _ in range(1000):
    batch_xs, batch_ys = mnist.train.next_batch(100)
    sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys})
{% endhighlight %}    

<p>
    Finally you test <u>other examples</u> you've not already used to learn to show if our model has the ability to 
    predict datas it has never seen before :
</p>

{% highlight python linenos %}
  correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))
  accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))
  print(sess.run(accuracy, feed_dict={x: mnist.test.images,
                                      y_: mnist.test.labels}))
{% endhighlight %}      

<p>
    Results here :
    <br> <img src="{{ site.baseurl }}/img/2016-12-10-tf-mnist/tf-exec.png" alt="results">
    <br> We built a model with 91.9% accuracy, in less than 20 lines of code, and with the most basic neural net (only 2 layers !)... not bad.
    <br> In a future post we will see how we can reach the accuracy to 99% !

</p>           

<p>
    I admit, this <a href="https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/tutorials/mnist/mnist_softmax.py">code is</a> the actual TensorFlow "hello world" tutorial.
    Thanks to them.
</p>                     












