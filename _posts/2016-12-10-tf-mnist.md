---
layout:     post
title:      "Image Recognition with Tensorflow"
subtitle:   "in Python"
date:       2016-12-10 12:00:00
author:     "Exced"
header-img: "img/2016-12-06-im-alive/bg.png"
---

<p>
    We are going to build a simple neural network to recognize handwritten digits (MNIST database) with Tensorflow.
    <br> We'll solve this problem step by step pointing out the involved problems and how to solve it.
    <br> We'll use different activation functions, different models to build a model with 99 % accuracy !
</p>

<p>
    This is a <b>classification problem</b> : we learn from datas and then try to guess the class of a new data.
    <br> Since we know the class of each data, we say that we make a <u>supervised learning</u> algorithm.
</p>

<h3 class="subsection-heading"> Datas </h3>

<p>
    We'll use the famous MNIST handwritten digits datasets. There are 60,000 training examples and 10,000 test examples.
    <br> You can find the <a href="http://yann.lecun.com/exdb/mnist/">MNIST dataset on Yann Lecun site.</a> 
    <br> The images are all 28x28 representing numbers from 0 to 9.        
</p>
<p>
    The classes of data are digits from 0 to 9. Given a new data we will try to recognize which digit it represents.
    <img src="{{ site.baseurl }}/img/2016-12-10-tf-mnist/mnist-2.png" alt="mnist 2" height="100" width="100">
    It's a 2 !
</p>

<h3 class="subsection-heading"> Let's build our model ! </h3>

<p>
    Since we want to give the class of the digit, we need an output layer of 10 coefficients, each of it telling us the probability of being 0 or 1 or 2...
    <br>We will consider <b>all pixels</b> of the image : a 28x28 pixel matrix.
</p>
<p>
    The first idea is to consider all pixels of the image. If digits are written by a perfect printer, they are all the same, then we say : 
    <u>this pixel</u> is significant to describe <u>this digit</u>. 
    <br> We want to find a way to discriminate all digits : we define a <b>weight matrix</b>.
    <br> We introduce a bias, because there are lot of ways to write a digit (we lost our magic printer). 
</p>
<p>
    We now have a model, that take the input value, consider a bias, and calculates the probability of having one digit.
    <br> Given Y as output vector, X as input vector, W the weight matrix and b a bias vector :
    <br> $$ Y\_ = X.W + b $$
    <br> (you've probably noticed the '_' after 'Y'. It represents the presumed output vector, that will be compared to the 
    real output 'Y', since we know the stuff in this supervised world).
</p>

<h4 class="subsubsection-heading"> Softmax </h4>
<p>
    Softmax is a simple function that squashes the range of a vector to [0,1]. It's easier to deal with probabilities with it.
    <br> $$ Y\_ = softmax(X.W + b) $$
</p>

<h4 class="subsubsection-heading"> Gradient descent </h4>
<p>
    Gradient descent is an algorithm to minimize a function of a Hilbert space.
    <br> Intuitively, if you want to minimize a function, the shortest path is to go to the opposite of the local gradient.
    <br> At each iteration we go to :
    <br> $$ x_{k+1} = x_k - \alpha_k \nabla f(x_k) $$
</p>

<h3 class="subsection-heading"> Code </h3>

{% highlight python %}
# input X: 28x28 grayscale images, the first dimension (None) will index the images in the mini-batch
X = tf.placeholder(tf.float32, [None, 28, 28, 1])
# correct answers will go here
Y_ = tf.placeholder(tf.float32, [None, 10])
# weights W[784, 10]   784=28*28
W = tf.Variable(tf.zeros([784, 10]))
# biases b[10]
b = tf.Variable(tf.zeros([10]))

# flatten the images into a single line of pixels
# -1 in the shape definition means "the only possible dimension that will preserve the number of elements"
XX = tf.reshape(X, [-1, 784])
{% endhighlight %}












