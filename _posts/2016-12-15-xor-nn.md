---
layout:     post
title:      "XOR Neural Network"
subtitle:   "in Python"
date:       2016-12-15 12:00:00
author:     "Exced"
header-img: "img/home-bg.png"
---

<p>
    We are going to build a simple neural network from scratch to recognize XOR function.
    This is the "hello world" equivalent of machine learning, and we'll build it using TensorFlow
</p>

<p>
Here is what we want to be able to learn, giving 2 vectors A and B :
<table>
  <tr>
    <th>A</th>
    <th>B</th>
    <th>A XOR B</th>
  </tr>
  <tr>
    <td>0</td>
    <td>0</td>
    <td>0</td>
  </tr>
  <tr>
    <td>0</td>
    <td>1</td>
    <td>1</td>
  </tr>
  <tr>
    <td>1</td>
    <td>0</td>
    <td>1</td>
  </tr>
  <tr>
    <td>1</td>
    <td>1</td>
    <td>0</td>
  </tr>
</table>    

Since we know the real outputs of the function, we say that we do <b>supervised learning</b>
<br> 
</p>

<h3 class="subsection-heading"> Neural Net model </h3>

<p>
Input vector is all the combinations of 2 bits ([0,0];[0,1];[1,0];[1,1]).
Output vector is a number between 0 and 1 that represents the probability of being a 0 or 1.
Here is the neural net :

<img class="center-image" src="{{ site.baseurl }}/img/2016-12-15-xor-nn/xor_nn_model.png" alt="XOR neural net model" height="400" width="400">


</p>

<h3 class="subsection-heading"> Code </h3>

<h4 class="subsubsection-heading"> Activation functions </h4>
<p>

We do a logistic regression so we'll use a classical linear function : y = ax + b.

Now have a look at 2 functions : tanh and sigmoid that transform our output values to probabilities.

<h5 class="subsubsubsection-heading"> tanh </h5>

<a href="https://commons.wikimedia.org/wiki/File:Hyperbolic_Tangent.svg#/media/File:Hyperbolic_Tangent.svg">
<img class="center-image" src="https://upload.wikimedia.org/wikipedia/commons/thumb/8/87/Hyperbolic_Tangent.svg/1200px-Hyperbolic_Tangent.svg.png" alt="Hyperbolic Tangent.svg" height="400" width="400">
</a> 
</p>
{% highlight python %}
import numpy as np

def tanh(x):
    return np.tanh(x)

def tanh_p(x):
    return 1.0 - x**2
{% endhighlight %}

<h5 class="subsubsubsection-heading"> sigmoid </h5>

<a href="https://commons.wikimedia.org/wiki/File:Sigmoide.PNG#/media/File:Sigmoide.PNG">
<img class="center-image" src="https://upload.wikimedia.org/wikipedia/commons/9/9d/Sigmoide.PNG" alt="Sigmoide.PNG" height="400" width="400">
</a>

{% highlight python %}
def sigmoid(x):
    return 1.0/(1.0 + np.exp(-x))

def sigmoid_p(x):
    return sigmoid(x)*(1.0-sigmoid(x))
{% endhighlight %}

<p>
We can see see that these 2 functions are similarly good to discriminate values to -1 and +1.
</p>

<h3 class="subsection-heading"> TensorFlow </h3>

<h5 class="subsubsubsection-heading"> create the model </h5>
<p>
Placeholders are memory emplacement which will be feed during the program.
<br> Variables will be define during the TF session.
<br>We also use the built-in sigmoid function of TensorFlow.
</p>


{% highlight python %}
  import tensorflow as tf

  x_ = tf.placeholder(tf.float32, [4, 2]) #correct input
  y_ = tf.placeholder(tf.float32, [4, 1]) #correct output
  b1 = tf.Variable(tf.zeros([2])) #bias
  b2 = tf.Variable(tf.zeros([1])) #bias 
  t1 = tf.Variable(tf.random_uniform([2,2], -1, 1)) #theta1
  t2 = tf.Variable(tf.random_uniform([2,1], -1, 1)) #theta2
  layer2 = tf.sigmoid(tf.matmul(x_, t1) + b1)
  y = tf.sigmoid(tf.matmul(layer2, t2) + b2) #training output
{% endhighlight %}

<h5 class="subsubsubsection-heading"> gradient descent </h5>

<p>
  Use a loss function : l2 norm works well and minimize it through gradient descent algorithm.
</p>
{% highlight python %}
  loss = tf.nn.l2_loss(y_ - y)
  train_step = tf.train.GradientDescentOptimizer(0.5).minimize(loss)
{% endhighlight %}

<h5 class="subsubsubsection-heading"> train the model </h5>
{% highlight python %}
  for i in range(10000):
    sess.run(train_step, feed_dict={x_: [[0,0],[0,1],[1,0],[1,1]], y_: [[0],[1],[1],[0]]})
{% endhighlight %}

<h5 class="subsubsubsection-heading"> All the code </h5>
{% highlight python %}
from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import tensorflow as tf

def main(_):

  # Create the model
  x_ = tf.placeholder(tf.float32, [4, 2]) #correct input
  y_ = tf.placeholder(tf.float32, [4, 1]) #correct output
  b1 = tf.Variable(tf.zeros([2])) #bias
  b2 = tf.Variable(tf.zeros([1])) #bias 
  t1 = tf.Variable(tf.random_uniform([2,2], -1, 1)) #theta1
  t2 = tf.Variable(tf.random_uniform([2,1], -1, 1)) #theta2
  layer2 = tf.sigmoid(tf.matmul(x_, t1) + b1)
  y = tf.sigmoid(tf.matmul(layer2, t2) + b2) #training output

  loss = tf.nn.l2_loss(y_ - y)
  train_step = tf.train.GradientDescentOptimizer(0.5).minimize(loss)

  sess = tf.InteractiveSession()
  tf.global_variables_initializer().run()
  # Train
  for i in range(10000):
    sess.run(train_step, feed_dict={x_: [[0,0],[0,1],[1,0],[1,1]], y_: [[0],[1],[1],[0]]})
    if i % 100 == 0:
		  print('y ', sess.run(y, feed_dict={x_: [[0,0],[0,1],[1,0],[1,1]], y_: [[0],[1],[1],[0]]}))
		  print('cost ', sess.run(cost, feed_dict={x_: [[0,0],[0,1],[1,0],[1,1]], y_: [[0],[1],[1],[0]]}))

if __name__ == '__main__':
  tf.app.run(main=main)
{% endhighlight %}    


<p>
You can fin the code on Github <a href="https://github.com/exced/xor_neural_net">here</a>.
</p>










